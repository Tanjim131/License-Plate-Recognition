{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License Plate Recognition using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Tanjim Bin Faruk, December 12, 2022* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💻 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it was time to submit an idea for the CS 545 term project, I had watched [a video on YouTube](https://www.youtube.com/watch?v=4AnyhHl3_tE) around that time. It was about Dashcam footage quality. The video went on to describe how the video quality of every dashcam is subpar, no matter what the cost. It piqued my interest and I decided why not build something that can possibly detect license plate number from dashcam footage.\n",
    "\n",
    "I searched around and found an interesting dataset on Kaggle. I also heard about object detection models in the passing during my undergrad, specifically YOLO models. I decided to use them for this project.\n",
    "\n",
    "I was initially eager to use the CNN model that we built for Assignment $5$, but decided not to go down that path as the accuracy for that model was not that great. I focused on OCRs instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Methods\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📂 Dataset\n",
    "\n",
    "I downloaded a public domain dataset from Kaggle [Maranhão, 2019]. The dataset contains $433$ images of license plate, which on the first look, appear to be from a number of different countries. The dataset contained two folders: *Images* and *Annotations*. The *Images* folder contained the *\".jpg\"* files for license plates. For each of the JPG files in the *Images* folder, the *Annotation* folder contained an \"*.xml\"* file. The XML files contained the bounding box coordinates of the license plate in the images.\n",
    "\n",
    "\n",
    "### 🪛 License Plate Detection Algorithm\n",
    "\n",
    "I decided to use YOLOv5 [Jocher, et al., 2020] which is an acronym for 'You only look once'. YOLO is one of the most famous object detection algorithms due to its speed and accuracy. It is a state of the art object detection algorithm that divides images into a grid system. It is a pre-trained model trained on the [MS COCO](https://cocodataset.org/#home) dataset, which includes $80$ categories of object. As the categories of objects did not include license plates, I needed to retrain the model on a custom license plate dataset. As YOLOv5 is a pre-trained model, using transfer learning, it would converge much faster and would have much better accuracy than say a CNN model written from scratch. Using a scratch CNN model would result in a lot of time spent for fine tuning the parametes to arrive at a satisfactory result.\n",
    "\n",
    "### 📥 Data Preprocessing\n",
    "\n",
    "In order to pass my dataset to the YOLOv5 model, I needed to restructure the folder hierarchy for *Images* and *Annotations*. Following [Train Custom Data on YOLOv5](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data), I first needed to create a `dataset.yaml` file which would contain the directory path for train, validation and test folders as well as key-value pair(s) for each of the classes. As we are only detecting license plates, our `dataset.yaml` would contain only one key-value pair.\n",
    "\n",
    "YOLOv5 requires a label for each images in the form a text document. As our downloaded dataset contains the annotations in XML files, I extracted the bounding box information from XML files and put them in a `labels` directory beside `images` folder.\n",
    "\n",
    "### 📀 Train-Validation-Test Split\n",
    "\n",
    "After restructuring the folders, I partitioned the whole dataset into train, validation and test sets. Initial $80\\%$ data was earmarked for train set. The remaining $20\\%$ was split evenly between the validation and test set.\n",
    "\n",
    "### 📲 YOLOv5 Training\n",
    "\n",
    "To train the model, we need to run the `train.py` file inside `yolov5` directory. There are a number of parameters to specify:\n",
    "\n",
    "```python\n",
    "!cd yolov5-license-plate && python train.py --batch 16 --epochs 50 --data dataset.yml --weights yolov5s.pt --workers 2\n",
    "```\n",
    "\n",
    "- ***Batch size***: I specified the batch size to be $16$. Increasing the batch size to $64$ resulted in exhausting my GPU memory and halting the training process.\n",
    "- ***Epochs***: I decided to go with $50$ epochs as there were quite a few files to be trained on. I assume increasing the number of epochs would lead to a better performance, but there could be diminishing returns.\n",
    "- ***Weights***: There are $4$ models to choose from: `yolo5s` (small), `yolov5m` (medium), `yolov5l` (large) and `yolov5x` (extra large). The size and complexity of these models increases in the ascending order. As my workflow did not require complex setup, I chose `yolov5s`.\n",
    "- ***Workers***: Number of CPU workers. I specified it to be $2$.\n",
    "\n",
    "Apart from these parameters, there are two other important files. One is used for the specifying the model architecture and the other can be used to fine tune the hyper-parameters.\n",
    "\n",
    "Under `yolov5 > models` directory, the `yolov5s.yaml` contains the following architecture:\n",
    "\n",
    "```yaml\n",
    "# Parameters\n",
    "nc: 80  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 v6.0 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, C3, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 6, C3, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, C3, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 3, C3, [1024]],\n",
    "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 v6.0 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, C3, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n",
    "\n",
    "```\n",
    "\n",
    "Inside the YAML file, $3$ subdivisions are noticed: Anchors, Backbone, and Head. According to [Anchor Boxes for Object Detection](https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html):\n",
    "\n",
    "> Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets.\n",
    "\n",
    "The Backbone part mainly consists of Convolutional Neural Network, C3 (Concentrated-Comprehensive Convolution), and SPPF (Spatial Pixel Pair Features) modules for feature extraction. The Head network performs target prediction and passes the predicted output. \n",
    "\n",
    "Under `yolov5 > data > hyps` directory, `hyp.scratch-med.yaml` contains the following:\n",
    "\n",
    "```yaml\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.3  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 0.7  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.9  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.1  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)\n",
    "```\n",
    "\n",
    "The hyperparameter config file contains quite a few important fields such as:\n",
    "\n",
    "- Learning Rate\n",
    "- Momentum\n",
    "- Weight Decay\n",
    "- Parameters for Box Loss (Mean Squared Error), Objective Loss (Binary Cross Entropy) and Class Loss (Cross Entropy)\n",
    "- Hue, Saturation, Value parameters\n",
    "- Scale, Rotation and Translation parameters\n",
    "\n",
    "\n",
    "### 📑 Text Extraction\n",
    "\n",
    "Given a picture, our trained YOLOv5 model narrows down to a region of interest (ROI) that likely contains the license plate. I thought of two ways to extract the text: one with Segmented Characters and the other with Whole Image.\n",
    "\n",
    "#### 📥 Image Preprocessing\n",
    "\n",
    "Before passing the image through an OCR, I preprocessed the image. First I converted the image to Grayscale. Then, I passed it through a Bilateral Filter to reduce the noise. I also inverted the image and applied thresholding to have white text on black background.\n",
    "\n",
    "#### 📋 Optical Character Recognition (OCR)\n",
    "\n",
    "In order to obtain the license plate number as text, I employed OCR techniques. I decided to use `easyocr` module for this, which is a lightweight and efficient OCR. I also played around with `pytesseract`, but found EasyOCR's performance much better.\n",
    "\n",
    "#### 📷 Character Segmentation with Contour Detection\n",
    "\n",
    "After finding a ROI from YOLOv5 model, I passed the ROI through EasyOCR for a first pass, to focus on a large block of text in the middle of the image. The result was that it further narrowed down the initial ROI. After that, I did contour detection to extract individual characters as images. Finally, I did a second pass through EasyOCR on these images to figure out the character and concatenate them.\n",
    "\n",
    "#### 💡Whole Image Detection\n",
    "\n",
    "Unlike character segmentation, I only did one single pass to extract the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚩 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information for each training is saved under `yolov5 > runs > train > exp` directory. After the training was completed, a bunch of graphs were generated and by looking at them I could tell that the model training went well.\n",
    "\n",
    "### 📌 Precision-Recall Curve\n",
    "\n",
    "According to [Precision-Recall Curves by Doug Steen](https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248), the performance of PR curves can be compared in the following way:\n",
    "\n",
    "[<img alt = \"PR Curve Performance\" src=\"https://miro.medium.com/max/720/1*6QPLsDvjo4H6OZrxEBI8Fg.webp\">]\n",
    "\n",
    "The following graph shows that our trained model nearly reached the perfect classifier performance.\n",
    "\n",
    "[<img alt = \"Precision-Recall Curve\" src=\"yolov5-license-plate/runs/train/exp/PR_curve.png\" width = 1000/>](yolov5-license-plate/runs/train/exp/PR_curve.png)\n",
    "\n",
    "### 📌 F1 Curve\n",
    "\n",
    "Our model achieved a higher F1 score (closer to 1), which correlates to better performance.\n",
    "\n",
    "[<img alt = \"F1 Curve\" src=\"yolov5-license-plate/runs/train/exp/F1_curve.png\" width = 1000/>](yolov5-license-plate/runs/train/exp/PR_curve.png)\n",
    "\n",
    "### 📌 Metrics\n",
    "\n",
    "The following graph shows that the `box_loss` and `obj_loss` decreased with each iteration. The `cls_loss` is $0$ because there is no misclassification (as there is only one class).\n",
    "\n",
    "[<img alt = \"F1 Curve\" src=\"yolov5-license-plate/runs/train/exp/results.png\" width = 1000/>](yolov5-license-plate/runs/train/exp/results.png)\n",
    "\n",
    "### 📌 Prediction Results\n",
    "\n",
    "Let's take an example image from the test set.\n",
    "\n",
    "[<img alt = \"Test Image\" src=\"sample-test-car.png\">](sample-test-car.png)\n",
    "\n",
    "The model successfully narrowed down to the ROI.\n",
    "\n",
    "[<img alt = \"Test Image\" src=\"roi-sample.jpg\">](roi-sample.jpg)\n",
    "\n",
    "The OCR then further narrowed down the initial ROI.\n",
    "\n",
    "[<img alt = \"Test Image\" src=\"narrowed-down-roi-sample.jpg\">](narrowed-down-roi-sample.jpg)\n",
    "\n",
    "Which is quite hard to see so let's use Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fef10583a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "_image = cv2.imread(os.path.join('narrowed-down-roi-sample.jpg'))\n",
    "plt.imshow(_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, passing this image to OCR extracts the license plate number as text (***VIPER*** in this case) which is saved to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔖 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this project, I had only a vague idea about object detection models. I learned a great deal about object detection models and OCR in general. Preparing the data in the required format to pass it to the YOLOv5 model was another learning experience.\n",
    "\n",
    "The most difficult part was surprisingly the text extraction. Initially, I was under the impression that narrowing down to the license plate region would be hard and subsequent text extraction would be easy. It turned out to be exactly the opposite. I had play around quite a while to figure out how to extract the text. I tried a bunch of approaches:\n",
    "\n",
    "- Train a CNN on EMNIST dataset. It did not work out as expected possibly because the EMNIST dataset is mostly handwritten data but license plate characters are not.\n",
    "- Image Preprocessing was another difficult task. I had to tune the parameters in such a way so that the preprocessed image would be suitable for contour detection.\n",
    "- Detecting the contours took a long time. Primarily because I had to figure out how to focus on the core area on the license plate, ignoring all other contour noises.\n",
    "- Finally, even though the preprocessed segmented characters were quite legible, OCRs had a tough time recognizing them. I am not sure why but maybe a CNN would be better fitted for this task. I ran out of time for experiments but plan tweak this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  📖 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Jocher, et al., 2020] Glenn Jocher; Ayush Chaurasia; Alex Stoken; Jirka Borovec; NanoCode012; Yonghye Kwon; Kalen Michael; TaoXie; Jiacong Fang; imyhxy; Lorna; 曾逸夫(Zeng Yifu); Colin Wong; Abhiram V; Diego Montes; Zhiqiang Wang; Cristi Fati; Jebastin Nadar; Laughing; UnglvKitDe; Victor Sonck; tkianai; yxNONG; Piotr Skalski; Adam Hogan; Dhruv Nair; Max Strobel; Mrinal Jain, [YOLOv5 SOTA Realtime Instance Segmentation](https://github.com/ultralytics/yolov5), Zenodo, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T20:24:09.713104Z",
     "start_time": "2022-10-11T20:24:09.678375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Faruk-Project-Report.ipynb is 2146\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('Faruk-Project-Report.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGES_PATH = os.path.join('data', 'images')\n",
    "ANNOTATIONS_PATH = os.path.join('data', 'annotations')\n",
    "TRAIN_FOLDER = os.path.join('data', 'train')\n",
    "VALIDATION_FOLDER = os.path.join('data', 'validation')\n",
    "TEST_FOLDER = os.path.join('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN_FOLDER):\n",
    "    os.makedirs(TRAIN_FOLDER)\n",
    "\n",
    "if not os.path.exists(VALIDATION_FOLDER):\n",
    "    os.makedirs(VALIDATION_FOLDER)\n",
    "\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.makedirs(TEST_FOLDER)\n",
    "\n",
    "if not os.path.exists(f'{TRAIN_FOLDER}\\images'):\n",
    "    os.makedirs(f'{TRAIN_FOLDER}\\images')\n",
    "\n",
    "if not os.path.exists(f'{TRAIN_FOLDER}\\labels'):\n",
    "    os.makedirs(f'{TRAIN_FOLDER}\\labels')\n",
    "\n",
    "if not os.path.exists(f'{VALIDATION_FOLDER}\\images'):\n",
    "    os.makedirs(f'{VALIDATION_FOLDER}\\images')\n",
    "\n",
    "if not os.path.exists(f'{VALIDATION_FOLDER}\\labels'):\n",
    "    os.makedirs(f'{VALIDATION_FOLDER}\\labels')\n",
    "\n",
    "if not os.path.exists(os.path.join(TEST_FOLDER, 'images')):\n",
    "    os.makedirs(os.path.join(TEST_FOLDER, 'images'))\n",
    "\n",
    "if not os.path.exists(os.path.join(TEST_FOLDER, 'labels')):\n",
    "    os.makedirs(os.path.join(TEST_FOLDER, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data & kaggle datasets download -d andrewmvd/car-plate-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zipped_file = os.path.join('data', 'car-plate-detection.zip')\n",
    "\n",
    "shutil.unpack_archive(zipped_file, os.path.join('data'))\n",
    "os.remove(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'licence': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(IMAGES_PATH) if os.path.isfile(f'{IMAGES_PATH}\\{f}')]\n",
    "annotation_files = [f for f in os.listdir(ANNOTATIONS_PATH) if os.path.isfile(f'{ANNOTATIONS_PATH}\\{f}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_files_train, image_files_rem, annotation_files_train, annotation_files_rem = train_test_split(image_files, annotation_files, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files_validation, image_files_test, annotation_files_validation, annotation_files_test = train_test_split(image_files_rem, annotation_files_rem, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for (image_file_train, annotation_file_train) in zip(image_files_train, annotation_files_train):\n",
    "    image_file_train_path = f'{IMAGES_PATH}\\{image_file_train}'\n",
    "    annotation_file_train_path = f'{ANNOTATIONS_PATH}\\{annotation_file_train}'\n",
    "    Path(f'{image_file_train_path}').rename(f'{TRAIN_FOLDER}\\images\\{image_file_train}')\n",
    "    Path(f'{annotation_file_train_path}').rename(f'{TRAIN_FOLDER}\\labels\\{annotation_file_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (image_file_validation, annotation_file_validation) in zip(image_files_validation, annotation_files_validation):\n",
    "    image_file_validation_path = f'{IMAGES_PATH}\\{image_file_validation}'\n",
    "    annotation_file_validation_path = f'{ANNOTATIONS_PATH}\\{annotation_file_validation}'\n",
    "    Path(f'{image_file_validation_path}').rename(f'{VALIDATION_FOLDER}\\images\\{image_file_validation}')\n",
    "    Path(f'{annotation_file_validation_path}').rename(f'{VALIDATION_FOLDER}\\labels\\{annotation_file_validation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (image_file_test, annotation_file_test) in zip(image_files_test, annotation_files_test):\n",
    "    image_file_test_path = f'{IMAGES_PATH}\\{image_file_test}'\n",
    "    annotation_file_test_path = f'{ANNOTATIONS_PATH}\\{annotation_file_test}'\n",
    "    Path(f'{image_file_test_path}').rename(f'{TEST_FOLDER}\\images\\{image_file_test}')\n",
    "    Path(f'{annotation_file_test_path}').rename(f'{TEST_FOLDER}\\labels\\{annotation_file_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir(IMAGES_PATH)\n",
    "os.rmdir(ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "for annotation_file_train in annotation_files_train:\n",
    "    tree = etree.parse(os.path.join(TRAIN_FOLDER, 'labels', annotation_file_train))\n",
    "    file_name = Path(os.path.join(TRAIN_FOLDER, 'labels', annotation_file_train)).stem\n",
    "\n",
    "    image_width = int(tree.getroot().find('size').find('width').text)\n",
    "    image_height = int(tree.getroot().find('size').find('height').text)\n",
    "    \n",
    "    name = tree.getroot().find('object').find('name').text\n",
    "    object_class = labels_dict[name]\n",
    "    \n",
    "    bbox_xmin = int(tree.getroot().find('object').find('bndbox').find('xmin').text)\n",
    "    bbox_ymin = int(tree.getroot().find('object').find('bndbox').find('ymin').text)\n",
    "    bbox_xmax = int(tree.getroot().find('object').find('bndbox').find('xmax').text)\n",
    "    bbox_ymax = int(tree.getroot().find('object').find('bndbox').find('ymax').text)\n",
    "\n",
    "    bbox_x_center = (bbox_xmin + bbox_xmax) / 2\n",
    "    bbox_y_center = (bbox_ymin + bbox_ymax) / 2\n",
    "    bbox_width = bbox_xmax - bbox_xmin\n",
    "    bbox_height = bbox_ymax - bbox_ymin\n",
    "\n",
    "    normalized_x_center = bbox_x_center / image_width\n",
    "    normalized_y_center = bbox_y_center / image_height\n",
    "\n",
    "    normalized_bbox_width = bbox_width / image_width\n",
    "    normalized_bbox_height = bbox_height / image_height\n",
    "\n",
    "    row_text = f'{object_class} {normalized_x_center} {normalized_y_center} {normalized_bbox_width} {normalized_bbox_height}'\n",
    "\n",
    "    with open(f'{TRAIN_FOLDER}\\labels\\{file_name}.txt', 'w') as label:\n",
    "        label.write(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(TRAIN_FOLDER, 'labels')):\n",
    "    if Path(file).suffix == '.xml':\n",
    "        os.remove(os.path.join(TRAIN_FOLDER, 'labels', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation_file_validation in annotation_files_validation:\n",
    "    tree = etree.parse(os.path.join(VALIDATION_FOLDER, 'labels', annotation_file_validation))\n",
    "    file_name = Path(os.path.join(VALIDATION_FOLDER, 'labels', annotation_file_validation)).stem\n",
    "\n",
    "    image_width = int(tree.getroot().find('size').find('width').text)\n",
    "    image_height = int(tree.getroot().find('size').find('height').text)\n",
    "    \n",
    "    name = tree.getroot().find('object').find('name').text\n",
    "    object_class = labels_dict[name]\n",
    "    \n",
    "    bbox_xmin = int(tree.getroot().find('object').find('bndbox').find('xmin').text)\n",
    "    bbox_ymin = int(tree.getroot().find('object').find('bndbox').find('ymin').text)\n",
    "    bbox_xmax = int(tree.getroot().find('object').find('bndbox').find('xmax').text)\n",
    "    bbox_ymax = int(tree.getroot().find('object').find('bndbox').find('ymax').text)\n",
    "\n",
    "    bbox_x_center = (bbox_xmin + bbox_xmax) / 2\n",
    "    bbox_y_center = (bbox_ymin + bbox_ymax) / 2\n",
    "    bbox_width = bbox_xmax - bbox_xmin\n",
    "    bbox_height = bbox_ymax - bbox_ymin\n",
    "\n",
    "    normalized_x_center = bbox_x_center / image_width\n",
    "    normalized_y_center = bbox_y_center / image_height\n",
    "\n",
    "    normalized_bbox_width = bbox_width / image_width\n",
    "    normalized_bbox_height = bbox_height / image_height\n",
    "\n",
    "    row_text = f'{object_class} {normalized_x_center} {normalized_y_center} {normalized_bbox_width} {normalized_bbox_height}'\n",
    "\n",
    "    with open(f'{VALIDATION_FOLDER}\\labels\\{file_name}.txt', 'w') as label:\n",
    "        label.write(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(VALIDATION_FOLDER, 'labels')):\n",
    "    if Path(file).suffix == '.xml':\n",
    "        os.remove(os.path.join(VALIDATION_FOLDER, 'labels', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation_file_test in annotation_files_test:\n",
    "    tree = etree.parse(os.path.join(TEST_FOLDER, 'labels', annotation_file_test))\n",
    "    file_name = Path(os.path.join(TEST_FOLDER, 'labels', annotation_file_test)).stem\n",
    "\n",
    "    image_width = int(tree.getroot().find('size').find('width').text)\n",
    "    image_height = int(tree.getroot().find('size').find('height').text)\n",
    "    \n",
    "    name = tree.getroot().find('object').find('name').text\n",
    "    object_class = labels_dict[name]\n",
    "    \n",
    "    bbox_xmin = int(tree.getroot().find('object').find('bndbox').find('xmin').text)\n",
    "    bbox_ymin = int(tree.getroot().find('object').find('bndbox').find('ymin').text)\n",
    "    bbox_xmax = int(tree.getroot().find('object').find('bndbox').find('xmax').text)\n",
    "    bbox_ymax = int(tree.getroot().find('object').find('bndbox').find('ymax').text)\n",
    "\n",
    "    bbox_x_center = (bbox_xmin + bbox_xmax) / 2\n",
    "    bbox_y_center = (bbox_ymin + bbox_ymax) / 2\n",
    "    bbox_width = bbox_xmax - bbox_xmin\n",
    "    bbox_height = bbox_ymax - bbox_ymin\n",
    "\n",
    "    normalized_x_center = bbox_x_center / image_width\n",
    "    normalized_y_center = bbox_y_center / image_height\n",
    "\n",
    "    normalized_bbox_width = bbox_width / image_width\n",
    "    normalized_bbox_height = bbox_height / image_height\n",
    "\n",
    "    row_text = f'{object_class} {normalized_x_center} {normalized_y_center} {normalized_bbox_width} {normalized_bbox_height}'\n",
    "\n",
    "    with open(f'{TEST_FOLDER}\\labels\\{file_name}.txt', 'w') as label:\n",
    "        label.write(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(TEST_FOLDER, 'labels')):\n",
    "    if Path(file).suffix == '.xml':\n",
    "        os.remove(os.path.join(TEST_FOLDER, 'labels', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\CSU\\My-Courses\\Fall 2022\\CS 545 - Machine Learning\\Project\\license-plate\\yolov5-license-plate\\train.py\", line 55, in <module>\n",
      "    from utils.loggers import Loggers\n",
      "  File \"d:\\CSU\\My-Courses\\Fall 2022\\CS 545 - Machine Learning\\Project\\license-plate\\yolov5-license-plate\\utils\\loggers\\__init__.py\", line 12, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\", line 12, in <module>\n",
      "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py\", line 9, in <module>\n",
      "    from tensorboard.compat.proto.event_pb2 import SessionLog\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\event_pb2.py\", line 17, in <module>\n",
      "    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py\", line 17, in <module>\n",
      "    from tensorboard.compat.proto import histogram_pb2 as tensorboard_dot_compat_dot_proto_dot_histogram__pb2\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\histogram_pb2.py\", line 36, in <module>\n",
      "    _descriptor.FieldDescriptor(\n",
      "  File \"c:\\Users\\tanji\\anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 544, in __new__\n",
      "    _message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5-license-plate && python train.py --batch 16 --epochs 50 --data dataset.yml --weights yolov5s.pt --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\tanji/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "WARNING  'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING  'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n",
      "YOLOv5  2023-7-21 Python-3.9.13 torch-2.1.0.dev20230502+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "best_trained_model = torch.hub.load('ultralytics/yolov5', 'custom', path = 'yolov5-license-plate/runs/train/exp/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    converted = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.bilateralFilter(converted, 11, 17, 17)\n",
    "    inverted = cv2.bitwise_not(blur)\n",
    "    threshold = cv2.threshold(inverted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Morph open to remove noise and invert image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    invert = 255 - opening\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_region_of_interest(image):\n",
    "    results = best_trained_model(image) # .xyxy[0]\n",
    "\n",
    "    print(results)\n",
    "\n",
    "    results = results.xyxy[0]\n",
    "\n",
    "    roi = None\n",
    "\n",
    "    if results.any():\n",
    "        bounded_box = results[0].cpu().numpy()\n",
    "        \n",
    "        xmin = int(bounded_box[0])\n",
    "        xmax = int(bounded_box[2])\n",
    "        ymin = math.ceil(bounded_box[1])\n",
    "        ymax = math.ceil(bounded_box[3])\n",
    "\n",
    "        roi = image[ymin : ymax , xmin : xmax]\n",
    "    \n",
    "    return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "def narrow_down_region_of_interest(roi):    \n",
    "    preprocessed_roi = preprocess_image(roi)\n",
    "        \n",
    "    ocr_results = reader.readtext(preprocessed_roi)\n",
    "\n",
    "    narrowed_down_roi = None\n",
    "\n",
    "    for ocr_result in ocr_results:\n",
    "        image_area = roi.shape[0] * roi.shape[1]\n",
    "\n",
    "        bounded_box_points = ocr_result[0]\n",
    "\n",
    "        xmin = int(bounded_box_points[0][0])\n",
    "        ymin = int(bounded_box_points[1][1])\n",
    "        xmax = math.ceil(bounded_box_points[1][0])\n",
    "        ymax = math.ceil(bounded_box_points[2][1])\n",
    "\n",
    "        bounded_box_area = (xmax - xmin) * (ymax - ymin)\n",
    "\n",
    "        if bounded_box_area / image_area >= 0.3:\n",
    "            narrowed_down_roi = roi[ymin : ymax, xmin : xmax]\n",
    "\n",
    "    return narrowed_down_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_detection(narrowed_down_roi):\n",
    "    preprocessed_nd_roi = preprocess_image(narrowed_down_roi)\n",
    "    contours, _ = cv2.findContours(preprocessed_nd_roi, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    sorted_contours = sorted(contours, key = cv2.contourArea, reverse=True)\n",
    "\n",
    "    filtered_contours = []\n",
    "\n",
    "    for sorted_contour in sorted_contours:\n",
    "        image_h, image_w = preprocessed_nd_roi.shape[0], preprocessed_nd_roi.shape[1]\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(sorted_contour)\n",
    "\n",
    "        if x - 3 < 0 or y - 3 < 0: # disregard outer box contour\n",
    "            continue\n",
    "\n",
    "        if h / image_h < 0.5 or h / image_h > 0.95: # disregard very small and very large contours\n",
    "            continue\n",
    "        \n",
    "        filtered_contours.append(sorted_contour)\n",
    "\n",
    "    return sorted(filtered_contours, key=lambda x : cv2.boundingRect(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def extract_characters(narrowed_down_roi):\n",
    "    contours = contour_detection(narrowed_down_roi)\n",
    "\n",
    "    temp_dir = os.path.join('temp')\n",
    "\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    for index, contour in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        character_image = narrowed_down_roi[y - 1 : y + h + 1, x - 1 : x + w + 1] # buffer space\n",
    "        resized_character_image = cv2.resize(character_image, (28, 28))\n",
    "        cv2.imwrite(f'temp/{index}.png', character_image)\n",
    "\n",
    "    characters = []\n",
    "\n",
    "    for c in os.listdir(temp_dir):\n",
    "        cimg = cv2.imread(os.path.join(temp_dir, c))\n",
    "        preprocessed_img = preprocess_image(cimg)\n",
    "        results = reader.readtext(preprocessed_img)\n",
    "        \n",
    "        if results:\n",
    "            characters.append(results[0][1])\n",
    "\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    return ''.join(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(output_license_plate):\n",
    "    alphanumeric_license_plate = ''.join([c for c in output_license_plate if c.isalnum()])\n",
    "    return alphanumeric_license_plate.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import uuid\n",
    "\n",
    "def save_license_plate_information(license_plate_number, license_plate_image):\n",
    "    folder = os.path.join('Detection-Results')\n",
    "\n",
    "    image_name = f'{uuid.uuid1()}.jpg'\n",
    "\n",
    "    segmentation_plate_number = license_plate_number[0]\n",
    "    whole_plate_number = license_plate_number[1]\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    if os.path.exists(os.path.join('detections.csv')) is False:\n",
    "        with open('detections.csv', 'a') as f:\n",
    "            csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerow(['Image Name', 'Character Segmentation Recognition', 'Whole Image Recognition'])    \n",
    "\n",
    "    if license_plate_number:\n",
    "        cv2.imwrite(os.path.join(folder, image_name), license_plate_image)\n",
    "\n",
    "        with open('detections.csv', 'a') as f:\n",
    "            csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerow([image_name, segmentation_plate_number, whole_plate_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_plate_number(input_image):\n",
    "    roi = get_region_of_interest(input_image)\n",
    "\n",
    "    if roi is not None:\n",
    "        \n",
    "        narrowed_down_roi = narrow_down_region_of_interest(roi)\n",
    "\n",
    "        if narrowed_down_roi is not None:\n",
    "            character_segmentation_plate_number = clean_output(extract_characters(narrowed_down_roi))\n",
    "\n",
    "            whole_image_results = reader.readtext(preprocess_image(narrowed_down_roi))\n",
    "\n",
    "            whole_image_plate_number = ''\n",
    "\n",
    "            if whole_image_results:\n",
    "                whole_image_plate_number = clean_output(whole_image_results[0][1])\n",
    "\n",
    "            save_license_plate_information((character_segmentation_plate_number, whole_image_plate_number), narrowed_down_roi)\n",
    "\n",
    "            return character_segmentation_plate_number, whole_image_plate_number, narrowed_down_roi\n",
    "        return '', '', None\n",
    "    return '', '', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "  \n",
    "# cap = cv2.VideoCapture('test.mp4')\n",
    "# # cap = cv2.VideoCapture(0)\n",
    "  \n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     cs_plate_number, wi_plate_number, roi = get_plate_number(frame)\n",
    "\n",
    "#     if cs_plate_number == '' and wi_plate_number == '':\n",
    "#         cv2.imshow(\"License Plate Recognition\", frame)\n",
    "#     else:\n",
    "#         cv2.imshow(\"License Plate Recognition\", np.squeeze(best_trained_model(frame).render()))\n",
    "#         save_license_plate_information((cs_plate_number, wi_plate_number), roi)\n",
    "      \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "  \n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_image = cv2.imread('sample-test-car.png')\n",
    "print(get_plate_number(input_image)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "276b36899633141f9bb9c26855a2fb39641ac12e40325f80aabae73e8e8011d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
